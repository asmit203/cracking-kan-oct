{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sympy as sym\n",
    "import sklearn as sk\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NORMAL18.jpeg</td>\n",
       "      <td>/Users/asmitganguly/Developer/internships/UWat...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DR106.jpeg</td>\n",
       "      <td>/Users/asmitganguly/Developer/internships/UWat...</td>\n",
       "      <td>Diabetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMRD44.jpeg</td>\n",
       "      <td>/Users/asmitganguly/Developer/internships/UWat...</td>\n",
       "      <td>Age_related_Macular_Degeneration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSR37.jpeg</td>\n",
       "      <td>/Users/asmitganguly/Developer/internships/UWat...</td>\n",
       "      <td>Central_serous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NORMAL110.jpeg</td>\n",
       "      <td>/Users/asmitganguly/Developer/internships/UWat...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_name                                               path  \\\n",
       "0   NORMAL18.jpeg  /Users/asmitganguly/Developer/internships/UWat...   \n",
       "1      DR106.jpeg  /Users/asmitganguly/Developer/internships/UWat...   \n",
       "2     AMRD44.jpeg  /Users/asmitganguly/Developer/internships/UWat...   \n",
       "3      CSR37.jpeg  /Users/asmitganguly/Developer/internships/UWat...   \n",
       "4  NORMAL110.jpeg  /Users/asmitganguly/Developer/internships/UWat...   \n",
       "\n",
       "                              class  \n",
       "0                            Normal  \n",
       "1                          Diabetic  \n",
       "2  Age_related_Macular_Degeneration  \n",
       "3                    Central_serous  \n",
       "4                            Normal  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"./test_data.csv\")\n",
    "df_train = pd.read_csv(\"./train_data.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMRD46.jpeg</td>\n",
       "      <td>/Users/asmitganguly/Developer/internships/UWat...</td>\n",
       "      <td>Age_related_Macular_Degeneration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CSR57.jpeg</td>\n",
       "      <td>/Users/asmitganguly/Developer/internships/UWat...</td>\n",
       "      <td>Central_serous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR27.jpeg</td>\n",
       "      <td>/Users/asmitganguly/Developer/internships/UWat...</td>\n",
       "      <td>Diabetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSR9.jpeg</td>\n",
       "      <td>/Users/asmitganguly/Developer/internships/UWat...</td>\n",
       "      <td>Central_serous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DR19.jpeg</td>\n",
       "      <td>/Users/asmitganguly/Developer/internships/UWat...</td>\n",
       "      <td>Diabetic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_name                                               path  \\\n",
       "0  AMRD46.jpeg  /Users/asmitganguly/Developer/internships/UWat...   \n",
       "1   CSR57.jpeg  /Users/asmitganguly/Developer/internships/UWat...   \n",
       "2    DR27.jpeg  /Users/asmitganguly/Developer/internships/UWat...   \n",
       "3    CSR9.jpeg  /Users/asmitganguly/Developer/internships/UWat...   \n",
       "4    DR19.jpeg  /Users/asmitganguly/Developer/internships/UWat...   \n",
       "\n",
       "                              class  \n",
       "0  Age_related_Macular_Degeneration  \n",
       "1                    Central_serous  \n",
       "2                          Diabetic  \n",
       "3                    Central_serous  \n",
       "4                          Diabetic  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M2 Pro) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 49 key-value pairs and 273 tensors from /Users/asmitganguly/Library/Caches/llama.cpp/ggml-org_SmolVLM-256M-Instruct-GGUF_SmolVLM-256M-Instruct-Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = SmolVLM 256M Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = SmolVLM\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 256M\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 2\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = SmolLM2 135M Instruct\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/HuggingFaceTB/...\n",
      "llama_model_loader: - kv  11:                  general.base_model.1.name str              = Siglip Base Patch16 512\n",
      "llama_model_loader: - kv  12:               general.base_model.1.version str              = 512\n",
      "llama_model_loader: - kv  13:          general.base_model.1.organization str              = Google\n",
      "llama_model_loader: - kv  14:              general.base_model.1.repo_url str              = https://huggingface.co/google/siglip-...\n",
      "llama_model_loader: - kv  15:                      general.dataset.count u32              = 2\n",
      "llama_model_loader: - kv  16:                     general.dataset.0.name str              = The_Cauldron\n",
      "llama_model_loader: - kv  17:             general.dataset.0.organization str              = HuggingFaceM4\n",
      "llama_model_loader: - kv  18:                 general.dataset.0.repo_url str              = https://huggingface.co/HuggingFaceM4/...\n",
      "llama_model_loader: - kv  19:                     general.dataset.1.name str              = Docmatix\n",
      "llama_model_loader: - kv  20:             general.dataset.1.organization str              = HuggingFaceM4\n",
      "llama_model_loader: - kv  21:                 general.dataset.1.repo_url str              = https://huggingface.co/HuggingFaceM4/...\n",
      "llama_model_loader: - kv  22:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\n",
      "llama_model_loader: - kv  23:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  24:                          llama.block_count u32              = 30\n",
      "llama_model_loader: - kv  25:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv  26:                     llama.embedding_length u32              = 576\n",
      "llama_model_loader: - kv  27:                  llama.feed_forward_length u32              = 1536\n",
      "llama_model_loader: - kv  28:                 llama.attention.head_count u32              = 9\n",
      "llama_model_loader: - kv  29:              llama.attention.head_count_kv u32              = 3\n",
      "llama_model_loader: - kv  30:                       llama.rope.freq_base f32              = 100000.000000\n",
      "llama_model_loader: - kv  31:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  32:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  33:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  34:                           llama.vocab_size u32              = 49280\n",
      "llama_model_loader: - kv  35:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  36:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  37:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  38:                      tokenizer.ggml.tokens arr[str,49280]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  39:                  tokenizer.ggml.token_type arr[i32,49280]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  40:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  41:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  42:                tokenizer.ggml.eos_token_id u32              = 49279\n",
      "llama_model_loader: - kv  43:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  44:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  45:                    tokenizer.chat_template str              = <|im_start|>{% for message in message...\n",
      "llama_model_loader: - kv  46:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  47:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  48:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:   61 tensors\n",
      "llama_model_loader: - type q8_0:  212 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 165.24 MiB (8.51 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token:  49278 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token:  49274 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token:  49270 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token:  49269 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token:  49268 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token:  49264 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token:  49261 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token:  49257 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token:  49253 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token:  49252 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token:  49251 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token:  49249 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token:  49247 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token:  49245 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token:  49243 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token:  49240 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token:  49236 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token:  49234 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token:  49231 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token:  49230 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token:  49228 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token:  49226 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token:  49220 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token:  49219 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token:  49212 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token:  49211 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token:  49208 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token:  49205 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token:  49203 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token:  49201 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token:  49200 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token:  49196 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token:  49194 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token:  49192 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token:  49191 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token:  49186 '<row_6_col_4>' is not marked as EOG\n",
      "load: control token:  49185 '<row_6_col_3>' is not marked as EOG\n",
      "load: control token:  49180 '<row_5_col_4>' is not marked as EOG\n",
      "load: control token:  49179 '<row_5_col_3>' is not marked as EOG\n",
      "load: control token:  49178 '<row_5_col_2>' is not marked as EOG\n",
      "load: control token:  49174 '<row_4_col_4>' is not marked as EOG\n",
      "load: control token:  49172 '<row_4_col_2>' is not marked as EOG\n",
      "load: control token:  49169 '<row_3_col_5>' is not marked as EOG\n",
      "load: control token:  49163 '<row_2_col_5>' is not marked as EOG\n",
      "load: control token:  49159 '<row_2_col_1>' is not marked as EOG\n",
      "load: control token:  49158 '<row_1_col_6>' is not marked as EOG\n",
      "load: control token:  49157 '<row_1_col_5>' is not marked as EOG\n",
      "load: control token:  49154 '<row_1_col_2>' is not marked as EOG\n",
      "load: control token:  49153 '<row_1_col_1>' is not marked as EOG\n",
      "load: control token:  49152 '<global-img>' is not marked as EOG\n",
      "load: control token:  49256 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token:  49242 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token:  49164 '<row_2_col_6>' is not marked as EOG\n",
      "load: control token:  49222 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token:  49167 '<row_3_col_3>' is not marked as EOG\n",
      "load: control token:  49224 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token:  49181 '<row_5_col_5>' is not marked as EOG\n",
      "load: control token:  49223 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token:  49225 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token:  49267 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token:  49279 '<end_of_utterance>' is not marked as EOG\n",
      "load: control token:  49277 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token:  49238 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token:  49217 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token:  49175 '<row_4_col_5>' is not marked as EOG\n",
      "load: control token:  49165 '<row_3_col_1>' is not marked as EOG\n",
      "load: control token:  49235 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token:  49176 '<row_4_col_6>' is not marked as EOG\n",
      "load: control token:  49161 '<row_2_col_3>' is not marked as EOG\n",
      "load: control token:  49210 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token:  49275 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token:  49160 '<row_2_col_2>' is not marked as EOG\n",
      "load: control token:  49276 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token:  49187 '<row_6_col_5>' is not marked as EOG\n",
      "load: control token:  49218 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token:  49183 '<row_6_col_1>' is not marked as EOG\n",
      "load: control token:  49155 '<row_1_col_3>' is not marked as EOG\n",
      "load: control token:  49195 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token:  49189 '<fake_token_around_image>' is not marked as EOG\n",
      "load: control token:  49193 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token:  49182 '<row_5_col_6>' is not marked as EOG\n",
      "load: control token:  49241 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token:  49255 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token:  49248 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token:     15 '<jupyter_script>' is not marked as EOG\n",
      "load: control token:      7 '<gh_stars>' is not marked as EOG\n",
      "load: control token:  49250 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token:  49199 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token:  49206 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token:  49263 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token:      5 '<file_sep>' is not marked as EOG\n",
      "load: control token:  49190 '<image>' is not marked as EOG\n",
      "load: control token:  49221 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token:  49204 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token:  49177 '<row_5_col_1>' is not marked as EOG\n",
      "load: control token:  49171 '<row_4_col_1>' is not marked as EOG\n",
      "load: control token:  49262 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token:  49214 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token:  49239 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token:  49173 '<row_4_col_3>' is not marked as EOG\n",
      "load: control token:  49237 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token:  49188 '<row_6_col_6>' is not marked as EOG\n",
      "load: control token:  49259 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token:  49198 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token:  49260 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token:  49232 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token:     12 '<jupyter_text>' is not marked as EOG\n",
      "load: control token:     10 '<issue_closed>' is not marked as EOG\n",
      "load: control token:  49271 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token:     11 '<jupyter_start>' is not marked as EOG\n",
      "load: control token:      1 '<|im_start|>' is not marked as EOG\n",
      "load: control token:  49168 '<row_3_col_4>' is not marked as EOG\n",
      "load: control token:      9 '<issue_comment>' is not marked as EOG\n",
      "load: control token:      8 '<issue_start>' is not marked as EOG\n",
      "load: control token:  49156 '<row_1_col_4>' is not marked as EOG\n",
      "load: control token:  49197 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token:  49273 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token:  49265 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token:  49216 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token:  49215 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token:  49229 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token:  49184 '<row_6_col_2>' is not marked as EOG\n",
      "load: control token:  49162 '<row_2_col_4>' is not marked as EOG\n",
      "load: control token:  49258 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token:  49244 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token:  49213 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token:  49202 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token:  49207 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token:  49166 '<row_3_col_2>' is not marked as EOG\n",
      "load: control token:  49246 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token:     13 '<jupyter_code>' is not marked as EOG\n",
      "load: control token:  49254 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token:  49227 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token:  49266 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token:      3 '<repo_name>' is not marked as EOG\n",
      "load: control token:  49272 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token:  49170 '<row_3_col_6>' is not marked as EOG\n",
      "load: control token:      6 '<filename>' is not marked as EOG\n",
      "load: control token:  49233 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token:  49209 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token:     14 '<jupyter_output>' is not marked as EOG\n",
      "load: control token:     16 '<empty_output>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 145\n",
      "load: token to piece cache size = 0.3199 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 576\n",
      "print_info: n_layer          = 30\n",
      "print_info: n_head           = 9\n",
      "print_info: n_head_kv        = 3\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 192\n",
      "print_info: n_embd_v_gqa     = 192\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 1536\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 100000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 162.97 M\n",
      "print_info: general.name     = SmolVLM 256M Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 49280\n",
      "print_info: n_merges         = 48900\n",
      "print_info: BOS token        = 1 '<|im_start|>'\n",
      "print_info: EOS token        = 49279 '<end_of_utterance>'\n",
      "print_info: EOT token        = 2 '<|im_end|>'\n",
      "print_info: UNK token        = 0 '<|endoftext|>'\n",
      "print_info: PAD token        = 2 '<|im_end|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM REP token    = 4 '<reponame>'\n",
      "print_info: EOG token        = 0 '<|endoftext|>'\n",
      "print_info: EOG token        = 2 '<|im_end|>'\n",
      "print_info: EOG token        = 4 '<reponame>'\n",
      "print_info: EOG token        = 49279 '<end_of_utterance>'\n",
      "print_info: max token length = 162\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 272 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/31 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =   165.24 MiB\n",
      "....................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 100000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x33a6129b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x12c09c7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x118122cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x118119b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x1024ef3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x12c09b670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x1024f2a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x118125290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x118122360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x33a618810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x118127f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x33a548e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x11b7ab890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x33a5498a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x338342e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x1024f9930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x3383451c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x12c09bcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x338d3b3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x338345a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x33a54a410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x338d3b8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x33a54ad70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x118128550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x118128a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x33a54b340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11b7ab430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x33a54b7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x33a54c050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x33a54ba00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c09cf50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x33a54c940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x12c09acc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x33a54d250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c09d9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c09e9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c09f360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c09fa40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11b7ac970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x33a54d970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x33a54cba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x33a618f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x33a6195b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x33a6170e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x33a617340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x33a54ea50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x33a54df60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x33a54f470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c0a0030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x3383460b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x33a54fbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x338d3bb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x338d3bf70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x338346c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x338343b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x338347940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x12c0a0630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x3383471f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x33a54f6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x11b7ad250 | th_max =  384 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x338d3c1d0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x338d3c430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11b7aa730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x338d3c690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x338347f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x338d3c8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x338348f20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x3383495e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x338349bb0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x33834a220 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x33834a960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x33834b040 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x33834b700 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x33a619b90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x33a550220 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c0a0c60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x33a5507b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c0a0ec0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12cf4e960 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11b7aa990 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12cf58320 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x118128c90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x33a550b30 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c0a1250 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11b7adbf0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11b7aec20 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c0a1e90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x33a551870 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x338d3cb50 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11b7af360 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11b7ae420 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11b7afaa0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x33834bbc0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x33a5512e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x338d3cde0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c0a2c50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x118128ef0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x338d3d070 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x338d3d2d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x338d3d560 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x33a551fb0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11b7b01c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x33a552690 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x33834c5f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x338d3d7f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x33834cd60 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x33a552aa0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c0a3390 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x338d3da80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x118129150 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c0a3e80 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c0a45e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12cf588b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118129440 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x33834de50 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x33a553760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x338d3dd10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x338d3df70 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x33834e590 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11b7b0420 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x338d3e1d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1181296a0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11b7b13b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c0a2590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c0a4df0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11b7b1b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x118129900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x33a5532d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x33a619f10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x104737d30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x33834d600 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105a35500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x338d3e430 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x33a553d90 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12cf58d20 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x33a553ff0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x33834ed80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x33a54ecb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11b7b2820 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c0a5d60 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x338d3e690 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x33a554af0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c0a64f0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x33834f860 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x33834ffb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x33834efe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11b7b2a80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c0a6c00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x338350e00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105913980 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11b7b3030 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x338d3e8f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1059152d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10580dd50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1059157d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x33a61b5c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x33a61c000 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x33a554440 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c0a7290 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11b7b3670 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x3383517e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11b7b3e90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x33a555330 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11b7b4550 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c0a7990 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c0a81a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11b7b4d60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x33a61a220 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x33834be20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c0a8930 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x338d3eb50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11b7b5490 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x338d3edb0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x3383513d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11b7b5c60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x338d3f010 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c0a90d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x33a555590 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x118129b60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x338352fc0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x338d3f270 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x338352740 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x338d3f4d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x338d3f730 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x33a555ad0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x33a556860 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x33a556200 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x33a556ff0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x33a5574e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x118129dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x33a61c260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x33a557d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x33a5583d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x11812a050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x33a558d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x338353fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11812a2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x33a61c4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x33a559640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x11812a510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11812a770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11812a9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x11812ac30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c0aa5c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c0a9f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x338d3fd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x33a559e60 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x338d3ffa0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c0aaec0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c0ab600 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x3383547e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x338353960 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x12c0abd90 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x33a55a0c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x338d40230 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11812ae90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c0ac530 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x338355750 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x338355e10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c0acc50 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x33a55b0b0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x338356560 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x338356d00 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x338357420 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x338d404c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c0ad3f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x338d40750 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c0adb10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11812b120 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x33a55a320 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x11812b380 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x33a5586e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x11b7b6440 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c0ae330 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11b7b6b90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x33a558940 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x33a55c2a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x33a61c720 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x105a31f50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x104739270 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104739860 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x33a61db00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x104739ac0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x33a55c880 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x33a55cea0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105a374a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105914250 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x33a55dc40 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x104738480 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105915e50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x11b7b72e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x33a55d4f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x338357cc0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x338358460 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12cf59490 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x338d409b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x11812b610 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x11b7b7540 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11b7b8580 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x338358bb0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x11b7b8cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x338359380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x11b7b9490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x338d40c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x338359ad0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x12c0aeb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x338d40e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11b7b9bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11b7ba320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10473a190 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x33835a140 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c0af1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x12c0afcd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x33a61cd20 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x338d410d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x338d41410 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x12c0af420 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x338d416b0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x33835a470 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x12c0b0460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x33835acc0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x33a55eb70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x33835b450 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x33835bbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c0b0cc0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12cf59810 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x33a55f2c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x33835c370 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x33835ca30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c0b24c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x33a55fd80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x11812b870 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x33a61e0a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x33a55ffe0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x11b7baa60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x11812bad0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x338d41940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x33a61f0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x338d41ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11b7bb1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c0b2f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11812bd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11b7bb8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11812bff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x33a560240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x33a5606f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11812c250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c0b1f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12c0b18b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12c0b34f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x33835d290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x33a560d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12c0b3bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12c0b42f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12c0b49d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x33835da00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12c0b50b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12c0b56a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x12c0b5c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x33a561340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x33835e390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x12c0b6530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x12c0b6d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x11b7bc770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x12c0b7460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x11b7bce10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x33a562120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x33a5627e0 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.19 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 30, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    45.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =   45.00 MiB, K (f16):   22.50 MiB, V (f16):   22.50 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =    98.50 MiB\n",
      "llama_context: graph nodes  = 1026\n",
      "llama_context: graph splits = 482 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"<|im_start|>{% for message in messages %}{{message['role'] | capitalize}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% endif %}{% endfor %}<end_of_utterance>\\n{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.value_length': '64', 'llama.rope.freq_base': '100000.000000', 'llama.attention.head_count_kv': '3', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '1536', 'llama.vocab_size': '49280', 'llama.embedding_length': '576', 'llama.context_length': '8192', 'general.size_label': '256M', 'general.base_model.0.name': 'SmolLM2 135M Instruct', 'llama.block_count': '30', 'tokenizer.ggml.eos_token_id': '49279', 'general.dataset.1.organization': 'HuggingFaceM4', 'llama.attention.key_length': '64', 'llama.attention.head_count': '9', 'general.dataset.1.name': 'Docmatix', 'general.dataset.count': '2', 'general.base_model.1.organization': 'Google', 'general.dataset.0.repo_url': 'https://huggingface.co/HuggingFaceM4/the_cauldron', 'general.dataset.0.organization': 'HuggingFaceM4', 'general.dataset.0.name': 'The_Cauldron', 'tokenizer.ggml.add_space_prefix': 'false', 'general.architecture': 'llama', 'general.base_model.1.version': '512', 'tokenizer.ggml.pre': 'smollm', 'general.base_model.1.name': 'Siglip Base Patch16 512', 'general.base_model.0.organization': 'HuggingFaceTB', 'general.base_model.count': '2', 'general.quantization_version': '2', 'llama.rope.dimension_count': '64', 'general.license': 'apache-2.0', 'general.base_model.0.repo_url': 'https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct', 'general.file_type': '7', 'general.finetune': 'Instruct', 'general.basename': 'SmolVLM', 'tokenizer.ggml.bos_token_id': '1', 'general.type': 'model', 'general.name': 'SmolVLM 256M Instruct', 'tokenizer.ggml.unknown_token_id': '0', 'general.dataset.1.repo_url': 'https://huggingface.co/HuggingFaceM4/Docmatix', 'general.base_model.1.repo_url': 'https://huggingface.co/google/siglip-base-patch16-512'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: <|im_start|>{% for message in messages %}{{message['role'] | capitalize}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% endif %}{% endfor %}<end_of_utterance>\n",
      "{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n",
      "Using chat eos_token: <end_of_utterance>\n",
      "Using chat bos_token: <|im_start|>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from llama_cpp import Llama\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the model\n",
    "llm = Llama(model_path=\"/Users/asmitganguly/Library/Caches/llama.cpp/ggml-org_SmolVLM-256M-Instruct-GGUF_SmolVLM-256M-Instruct-Q8_0.gguf\", n_ctx=2048)\n",
    "\n",
    "def image_to_base64_data_uri(file_path):\n",
    "    with open(file_path, \"rb\") as img_file:\n",
    "        base64_data = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "        return f\"data:image/png;base64,{base64_data}\"\n",
    "\n",
    "def construct_prompt(image_data_uri, classes):\n",
    "    class_list = ', '.join(classes)\n",
    "    prompt = (\n",
    "        f\"Classify the following image into one of the following classes: {class_list}.\\n\"\n",
    "        f\"![image]({image_data_uri})\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        buffered = BytesIO()\n",
    "        img.save(buffered, format=\"PNG\")\n",
    "        return base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "def classify_image(image_path, classes):\n",
    "    image_data_uri = image_to_base64_data_uri(image_path)\n",
    "    prompt = construct_prompt(image_data_uri, classes)\n",
    "    output = llm.create_completion(prompt=prompt)\n",
    "    return output['choices'][0]['text'].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Normal',\n",
       " 'Diabetic',\n",
       " 'Age_related_Macular_Degeneration',\n",
       " 'Central_serous',\n",
       " 'Macular_Hole']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = df_test['class'].unique().tolist()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (87125) exceed context window of 2048",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m image_path \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m true_label \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(predicted_label)\n\u001b[1;32m     11\u001b[0m true_labels\u001b[38;5;241m.\u001b[39mappend(true_label)\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mclassify_image\u001b[0;34m(image_path, classes)\u001b[0m\n\u001b[1;32m     31\u001b[0m image_data_uri \u001b[38;5;241m=\u001b[39m image_to_base64_data_uri(image_path)\n\u001b[1;32m     32\u001b[0m prompt \u001b[38;5;241m=\u001b[39m construct_prompt(image_data_uri, classes)\n\u001b[0;32m---> 33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx/lib/python3.12/site-packages/llama_cpp/llama.py:1835\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1835\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx/lib/python3.12/site-packages/llama_cpp/llama.py:1271\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mreset_timings()\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx:\n\u001b[0;32m-> 1271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1273\u001b[0m     )\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[0;31mValueError\u001b[0m: Requested tokens (87125) exceed context window of 2048"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Initialize liclassessts to store predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for index, row in tqdm(df_test.iterrows()):\n",
    "    image_path = row['path']\n",
    "    true_label = row['class']\n",
    "    predicted_label = classify_image(image_path, classes)\n",
    "    predictions.append(predicted_label)\n",
    "    true_labels.append(true_label)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

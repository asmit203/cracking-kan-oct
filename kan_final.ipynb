{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "### Need to run for longer epochs and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "        classes = list(pd.read_csv(\"/Users/asmitganguly/Developer/internships/UWaterloo-24/code/data/train_data.csv\")[\"class\"].unique())\n",
    "\n",
    "        # Initialize the label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(classes)\n",
    "\n",
    "        # Initialize the one hot encoder\n",
    "        onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "        # Create a dictionary to map the classes to the one hot encoding\n",
    "        class_dict = {}\n",
    "        for i in range(len(classes)):\n",
    "            class_dict[classes[i]] = onehot_encoded[i]\n",
    "            \n",
    "        self.class_dict = class_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_frame.iloc[idx]['path'])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.data_frame.iloc[idx]['class']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.class_dict[label]\n",
    "\n",
    "# Define the transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally with a probability of 0.5\n",
    "    transforms.RandomRotation(15),  # Randomly rotate the image by up to 15 degrees\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize([0.5], [0.5]),  # Normalize the image (mean and std for grayscale)\n",
    "    transforms.Lambda(lambda x: x + 0.01 * torch.randn_like(x))  # Add small random noise to the image\n",
    "])\n",
    "\n",
    "# Initialize the dataset\n",
    "TrainDataset = CustomImageDataset(csv_file='/Users/asmitganguly/Developer/internships/UWaterloo-24/code/data/train_data.csv', transform=data_transforms)\n",
    "TestDataset = CustomImageDataset(csv_file='/Users/asmitganguly/Developer/internships/UWaterloo-24/code/data/test_data.csv', transform=data_transforms)\n",
    "\n",
    "print(TrainDataset.__getitem__(0)[0].shape)\n",
    "print(TestDataset.__getitem__(1)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(TrainDataset, batch_size=4)\n",
    "valid_dataloader = DataLoader(TestDataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #! create the one hot encoding for the classes from the train datalaoader\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Get the classes from the dataloader\n",
    "# classes = list(pd.read_csv(\"/Users/asmitganguly/Developer/internships/UWaterloo-24/code/data/train_data.csv\")[\"class\"].unique())\n",
    "\n",
    "# # Initialize the label encoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# integer_encoded = label_encoder.fit_transform(classes)\n",
    "\n",
    "# # Initialize the one hot encoder\n",
    "# onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "# onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# # Create a dictionary to map the classes to the one hot encoding\n",
    "# class_dict = {}\n",
    "# for i in range(len(classes)):\n",
    "#     class_dict[classes[i]] = onehot_encoded[i]\n",
    "    \n",
    "# class_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm \n",
    "def train(net, train_dataloader, valid_dataloader, criterion, optimizer, scheduler=None, epochs=10, device='mps', checkpoint_epochs=10, chk_cont=None):\n",
    "    start = time.time()\n",
    "    if chk_cont is not None:\n",
    "        checkpoint = torch.load(chk_cont)\n",
    "        net.load_state_dict(checkpoint['state_dict'])\n",
    "        # optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start = time.time()\n",
    "        print(f'Checkpoint loaded from epoch {checkpoint[\"epoch\"]}')\n",
    "        \n",
    "    print(f'Training for {epochs} epochs on {device}')\n",
    "    start_epoch = 1\n",
    "    if chk_cont is not None:\n",
    "        print(f'Continuing training from epoch {checkpoint[\"epoch\"]}')\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    for epoch in range(start_epoch, epochs+1):\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        \n",
    "        net.train()  # put network in train mode for Dropout and Batch Normalization\n",
    "        train_loss = torch.tensor(0., device=device)  # loss and accuracy tensors are on the GPU to avoid data transfers\n",
    "        train_accuracy = torch.tensor(0., device=device)\n",
    "        for X, y in tqdm(train_dataloader):\n",
    "            X = X.to(torch.float32).to(device).mean(dim=1, keepdim=True)\n",
    "            \n",
    "            y = y.to(torch.float32).to(device)\n",
    "            preds = net(X)\n",
    "            # print(preds.shape)\n",
    "            loss = criterion(preds, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                train_loss += loss * train_dataloader.batch_size\n",
    "                train_accuracy += (torch.argmax(preds, dim=1) == torch.argmax(y, dim=1)).sum()\n",
    "        \n",
    "        if valid_dataloader is not None:\n",
    "            net.eval()  # put network in train mode for Dropout and Batch Normalization\n",
    "            valid_loss = torch.tensor(0., device=device)\n",
    "            valid_accuracy = torch.tensor(0., device=device)\n",
    "            with torch.no_grad():\n",
    "                for X, y in valid_dataloader:\n",
    "                    X = X.to(torch.float32).mean(dim=1, keepdim=True).to(device)\n",
    "                    y = y.to(torch.float32).to(device)\n",
    "                    preds = net(X)\n",
    "                    loss = criterion(preds, y)\n",
    "\n",
    "                    valid_loss += loss * valid_dataloader.batch_size\n",
    "                    valid_accuracy += (torch.argmax(preds, dim=1) == torch.argmax(y, dim=1)).sum()\n",
    "        \n",
    "        if scheduler is not None: \n",
    "            scheduler.step()\n",
    "            \n",
    "        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n",
    "        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n",
    "        \n",
    "        if valid_dataloader is not None:\n",
    "            print(f'Valid loss: {valid_loss/len(valid_dataloader.dataset):.2f}')\n",
    "            print(f'Valid accuracy: {100*valid_accuracy/len(valid_dataloader.dataset):.2f}')\n",
    "        \n",
    "        if epoch%checkpoint_epochs==0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, './weights/oct/checkpoint.pth.tar')\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Total training time: {end-start:.1f} seconds')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1x/ys4528cx32123ww1dck2swg40000gn/T/ipykernel_6306/1635648280.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_feature.load_state_dict(torch.load(\"./weights/Convs_and_KAN_pykan.pth\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from kan import KAN\n",
    "\n",
    "device='mps'\n",
    "class NormalConvsKAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NormalConvsKAN, self).__init__()\n",
    "        # Convolutional layer, assuming an input with 1 channel (grayscale image)\n",
    "        # and producing 16 output channels, with a kernel size of 3x3\n",
    "        self.conv1 = nn.Conv2d(1, 5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5, padding=1)\n",
    "        self.conv3 = nn.Conv2d(10, 20, kernel_size=7, padding=1)\n",
    "\n",
    "        # Max pooling layer\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.kan1 = KAN(\n",
    "            width=[20,10],\n",
    "            grid=10,\n",
    "            k=3,\n",
    "            noise_scale=0.01,\n",
    "            scale_base_mu=1,\n",
    "            scale_base_sigma=1,\n",
    "            grid_eps=0.02,\n",
    "            grid_range=[0,1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.kan1(x)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def feature(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "    def get_act(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class KAN_class(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KAN_class, self).__init__()\n",
    "        self.conv4 = nn.Conv2d(20, 40, kernel_size=9, padding=1)\n",
    "        self.conv5 = nn.Conv2d(40, 50, kernel_size=11, padding=1)\n",
    "\n",
    "        # Max pooling layer\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        \"\"\"self, width=None, grid=3, k=3, noise_scale=0.1, scale_base_mu=0.0, scale_base_sigma=1.0, base_fun=torch.nn.SiLU(), symbolic_enabled=True, bias_trainable=False, grid_eps=1.0, grid_range=[-1, 1], sp_trainable=True, sb_trainable=True, device='cpu', seed=0\"\"\"\n",
    "\n",
    "        self.kan1 = KAN(\n",
    "            width=[50,5],\n",
    "            grid=10,\n",
    "            k=3,\n",
    "            noise_scale=0.01,\n",
    "            scale_base_mu=1,\n",
    "            scale_base_sigma=1,\n",
    "            grid_eps=0.02,\n",
    "            grid_range=[0,1])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.kan1(x)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_feature = NormalConvsKAN()\n",
    "model_feature.load_state_dict(torch.load(\"./weights/Convs_and_KAN_pykan.pth\"))\n",
    "model_feature.to(device)\n",
    "model_class = KAN_class().to(device)\n",
    "\n",
    "# out = model_class(model_feature.feature(tt.to(device)))\n",
    "# print(\"out: \", out.shape)\n",
    "\n",
    "#new model as the mix of both the models\n",
    "class MixModel(nn.Module):\n",
    "    def __init__(self, model_feature, model_class):\n",
    "        super(MixModel, self).__init__()\n",
    "        self.model_feature = model_feature\n",
    "        self.model_class = model_class\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model_feature.feature(x)\n",
    "        x = self.model_class(x)\n",
    "        return x\n",
    "    \n",
    "model = MixModel(model_feature, model_class).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1x/ys4528cx32123ww1dck2swg40000gn/T/ipykernel_6306/99938414.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(chk_cont)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from epoch 100\n",
      "Training for 100 epochs on mps\n",
      "Continuing training from epoch 100\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 11/115 [00:04<00:43,  2.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m lr, weight_decay, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m, \u001b[38;5;241m5e-4\u001b[39m, \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DO_train:\n\u001b[0;32m----> 5\u001b[0m     net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchk_cont\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/asmitganguly/Developer/internships/UWaterloo-24/code/model/kan/weights/oct/checkpoint.pth.tar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/asmitganguly/Developer/internships/UWaterloo-24/code/model/kan/weights/oct/checkpoint.pth.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, train_dataloader, valid_dataloader, criterion, optimizer, scheduler, epochs, device, checkpoint_epochs, chk_cont)\u001b[0m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, y)\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/autograd/__init__.py:346\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/autograd/graph.py:812\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "lr, weight_decay, epochs = 1e-5, 5e-4, 100\n",
    "\n",
    "if DO_train:\n",
    "    net = train(model, train_dataloader, valid_dataloader, criterion, optimizer, None, epochs, device, chk_cont=\"/Users/asmitganguly/Developer/internships/UWaterloo-24/code/model/kan/weights/oct/checkpoint.pth.tar\")\n",
    "else:\n",
    "    checkpoint = torch.load(\"/Users/asmitganguly/Developer/internships/UWaterloo-24/code/model/kan/weights/oct/checkpoint.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f'Checkpoint loaded from epoch {checkpoint[\"epoch\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1x/ys4528cx32123ww1dck2swg40000gn/T/ipykernel_11263/2987727986.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/Users/asmitganguly/Developer/internships/UWaterloo-24/code/model/kan/weights/oct/checkpoint.pth.tar\")['state_dict'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/Users/asmitganguly/Developer/internships/UWaterloo-24/code/model/kan/weights/oct/checkpoint.pth.tar\")['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def test(net, train_dataloader, valid_dataloader, criterion, optimizer, scheduler=None, epochs=10, device='mps', checkpoint_epochs=10, chk=None):\n",
    "    start = time.time()\n",
    "    print(f'Training for {epochs} epochs on {device}')\n",
    "    if chk is not None:\n",
    "        checkpoint = torch.load(chk)\n",
    "        net.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        net.eval()\n",
    "        net.to(device)\n",
    "    results = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'predictions': [],\n",
    "        'targets': []\n",
    "    }\n",
    "    if valid_dataloader is not None:\n",
    "        net.eval()  # put network in train mode for Dropout and Batch Normalization\n",
    "        valid_loss = torch.tensor(0., device=device)\n",
    "        valid_accuracy = torch.tensor(0., device=device)\n",
    "        with torch.no_grad():\n",
    "            for X, y in valid_dataloader:\n",
    "                X = X.to(torch.float32).mean(dim=1, keepdim=True).to(device)\n",
    "                y = y.to(torch.float32).to(device)\n",
    "                preds = net(X)\n",
    "                loss = criterion(preds, y)\n",
    "\n",
    "                \n",
    "                \n",
    "                valid_loss += loss * valid_dataloader.batch_size\n",
    "                valid_accuracy += (torch.argmax(preds, dim=1) == torch.argmax(y, dim=1)).sum()\n",
    "                \n",
    "                all_predictions = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "                all_targets = torch.argmax(y, dim=1).cpu().numpy()\n",
    "                precision = precision_score(all_targets, all_predictions, average=None)\n",
    "                recall = recall_score(all_targets, all_predictions, average=None)\n",
    "                f1 = f1_score(all_targets, all_predictions, average=None)\n",
    "                \n",
    "                results['precision'].append(precision)\n",
    "                results['recall'].append(recall)\n",
    "                results['f1'].append(f1)\n",
    "                results['predictions'].append(all_predictions)\n",
    "                results['targets'].append(all_targets)\n",
    "    \n",
    "    if scheduler is not None: \n",
    "        scheduler.step()\n",
    "        \n",
    "    # print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n",
    "    # print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n",
    "    \n",
    "    if valid_dataloader is not None:\n",
    "        print(f'Valid loss: {valid_loss/len(valid_dataloader.dataset):.2f}')\n",
    "        print(f'Valid accuracy: {100*valid_accuracy/len(valid_dataloader.dataset):.2f}')\n",
    "    \n",
    "    # if epoch%checkpoint_epochs==0:\n",
    "    #     torch.save({\n",
    "    #         'epoch': epoch,\n",
    "    #         'state_dict': net.state_dict(),\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #     }, './weights/checkpoint.pth.tar')\n",
    "    \n",
    "    # print()\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'Total training time: {end-start:.1f} seconds')\n",
    "    return net, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1 epochs on mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1x/ys4528cx32123ww1dck2swg40000gn/T/ipykernel_11263/2623279554.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(chk)\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/asmitganguly/miniconda3/envs/mlx/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.64\n",
      "Valid accuracy: 86.09\n",
      "Total training time: 5.0 seconds\n",
      "{'precision': [array([0., 1., 0., 0., 1.]), array([1., 1., 1.]), array([0., 1., 1.]), array([1., 1., 0., 0.]), array([1., 1., 1.]), array([1., 0., 1.]), array([1., 1., 1.]), array([1., 1.]), array([1. , 0.5, 1. ]), array([0., 1., 1., 1.]), array([1., 1., 1., 1.]), array([1., 1., 1.]), array([1., 1.]), array([1. , 1. , 0.5]), array([1., 1.]), array([0., 1., 0.]), array([1.        , 0.66666667, 0.        ]), array([1., 1., 1.]), array([1., 1., 1.]), array([0.        , 1.        , 0.66666667]), array([1., 1., 1.]), array([0., 1., 1.]), array([1., 1.]), array([0.5, 0. , 1. ]), array([1., 1., 1.]), array([0., 0., 0., 1., 1.]), array([1., 1., 1.]), array([1., 1., 1.]), array([1., 1.])], 'recall': [array([0., 1., 0., 0., 1.]), array([1., 1., 1.]), array([0. , 0.5, 1. ]), array([1., 1., 0., 0.]), array([1., 1., 1.]), array([0.5, 0. , 1. ]), array([1., 1., 1.]), array([1., 1.]), array([0.5, 1. , 1. ]), array([0. , 1. , 1. , 0.5]), array([1., 1., 1., 1.]), array([1., 1., 1.]), array([1., 1.]), array([0.5, 1. , 1. ]), array([1., 1.]), array([0., 1., 0.]), array([1., 1., 0.]), array([1., 1., 1.]), array([1., 1., 1.]), array([0., 1., 1.]), array([1., 1., 1.]), array([0.        , 0.66666667, 1.        ]), array([1., 1.]), array([0.5, 0. , 1. ]), array([1., 1., 1.]), array([0., 0., 0., 1., 1.]), array([1., 1., 1.]), array([1., 1., 1.]), array([1., 1.])], 'f1': [array([0., 1., 0., 0., 1.]), array([1., 1., 1.]), array([0.        , 0.66666667, 1.        ]), array([1., 1., 0., 0.]), array([1., 1., 1.]), array([0.66666667, 0.        , 1.        ]), array([1., 1., 1.]), array([1., 1.]), array([0.66666667, 0.66666667, 1.        ]), array([0.        , 1.        , 1.        , 0.66666667]), array([1., 1., 1., 1.]), array([1., 1., 1.]), array([1., 1.]), array([0.66666667, 1.        , 0.66666667]), array([1., 1.]), array([0., 1., 0.]), array([1. , 0.8, 0. ]), array([1., 1., 1.]), array([1., 1., 1.]), array([0. , 1. , 0.8]), array([1., 1., 1.]), array([0. , 0.8, 1. ]), array([1., 1.]), array([0.5, 0. , 1. ]), array([1., 1., 1.]), array([0., 0., 0., 1., 1.]), array([1., 1., 1.]), array([1., 1., 1.]), array([1., 1.])], 'predictions': [array([4, 3, 2, 1]), array([4, 3, 1, 4]), array([2, 1, 4, 4]), array([0, 0, 4, 1]), array([4, 2, 4, 1]), array([4, 2, 3, 4]), array([4, 2, 3, 4]), array([1, 4, 4, 4]), array([4, 3, 3, 2]), array([1, 0, 3, 2]), array([0, 3, 4, 1]), array([2, 4, 1, 1]), array([3, 4, 4, 4]), array([0, 4, 1, 4]), array([4, 3, 4, 4]), array([3, 3, 3, 1]), array([2, 2, 1, 2]), array([1, 4, 3, 4]), array([4, 4, 0, 3]), array([4, 3, 4, 4]), array([4, 2, 4, 3]), array([0, 1, 1, 3]), array([1, 2, 2, 1]), array([4, 3, 2, 2]), array([1, 4, 2, 4]), array([1, 3, 2, 4]), array([4, 0, 2, 4]), array([2, 1, 3, 1]), array([4, 4, 0])], 'targets': [array([4, 2, 0, 1]), array([4, 3, 1, 4]), array([2, 2, 4, 4]), array([0, 0, 2, 1]), array([4, 2, 4, 1]), array([4, 2, 2, 4]), array([4, 2, 3, 4]), array([1, 4, 4, 4]), array([4, 2, 3, 2]), array([1, 3, 3, 2]), array([0, 3, 4, 1]), array([2, 4, 1, 1]), array([3, 4, 4, 4]), array([0, 0, 1, 4]), array([4, 3, 4, 4]), array([3, 3, 3, 4]), array([3, 2, 1, 2]), array([1, 4, 3, 4]), array([4, 4, 0, 3]), array([0, 3, 4, 4]), array([4, 2, 4, 3]), array([1, 1, 1, 3]), array([1, 2, 2, 1]), array([4, 2, 3, 2]), array([1, 4, 2, 4]), array([0, 3, 1, 4]), array([4, 0, 2, 4]), array([2, 1, 3, 1]), array([4, 4, 0])]}\n"
     ]
    }
   ],
   "source": [
    "net, results = test(net, train_dataloader, valid_dataloader, criterion, optimizer, None, 1, device,chk=\"/Users/asmitganguly/Developer/internships/UWaterloo-24/code/model/kan/weights/oct/checkpoint.pth.tar\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_final = {\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'predictions': [],\n",
    "    'targets': []\n",
    "}\n",
    "\n",
    "for k in results.keys():\n",
    "    for ll in results[k]:\n",
    "        for ele in ll:\n",
    "            result_final[k].append(ele)\n",
    "            \n",
    "# print(result_final)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(result_final[\"targets\"], result_final[\"predictions\"])\n",
    "\n",
    "# display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:04<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0060\n",
      "Test Accuracy of Age_related_Macular_Degeneration: 72% ( 8/11)\n",
      "Test Accuracy of Central_serous: 71% (15/21)\n",
      "Test Accuracy of Diabetic: 77% (17/22)\n",
      "Test Accuracy of Macular_Hole: 70% (14/20)\n",
      "Test Accuracy of Normal: 97% (40/41)\n",
      "Test Accuracy of 81% (94/115)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "classes = list(pd.read_csv(\"/Users/asmitganguly/Developer/internships/UWaterloo-24/code/data/train_data.csv\")[\"class\"].unique())\n",
    "class_correct = list(0 for i in range(len(classes)))\n",
    "class_total = list(0 for i in range(len(classes)))\n",
    "net.eval()\n",
    "\n",
    "for data, target in tqdm(valid_dataloader):\n",
    "    data, target = data.to(torch.float32).mean(dim=1, keepdim=True).to(device), target.argmax(dim=1).to(torch.int64).to(device)\n",
    "    with torch.no_grad(): # turn off autograd for faster testing\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "    test_loss = loss.item() * data.size(0)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # print(len(target))\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "test_loss = test_loss / (len(valid_dataloader)*4)\n",
    "print('Test Loss: {:.4f}'.format(test_loss))\n",
    "for i in range(len(classes)):\n",
    "    if class_total[i] > 0:\n",
    "        print(\"Test Accuracy of %5s: %2d%% (%2d/%2d)\" % (\n",
    "            classes[i], 100*class_correct[i]/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n",
    "        ))\n",
    "    else:\n",
    "        print(\"Test accuracy of %5s: NA\" % (classes[i]))\n",
    "print(\"Test Accuracy of %2d%% (%2d/%2d)\" % (\n",
    "            100*np.sum(class_correct)/np.sum(class_total), np.sum(class_correct), np.sum(class_total)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# def test(net, train_dataloader, valid_dataloader, criterion, optimizer, scheduler=None, epochs=10, device='mps', checkpoint_epochs=10):\n",
    "#     classes = list(pd.read_csv(\"/Users/asmitganguly/Developer/internships/UWaterloo-24/code/data/train_data.csv\")[\"class\"].unique())\n",
    "#     class_correct = list(0 for i in range(len(classes)))\n",
    "#     class_total = list(0 for i in range(len(classes)))\n",
    "#     start = time.time()\n",
    "#     print(f'Training for {epochs} epochs on {device}')\n",
    "#     results = {\n",
    "#         'precision': [],\n",
    "#         'recall': [],\n",
    "#         'f1': [],\n",
    "#         'predictions': [],\n",
    "#         'targets': []\n",
    "#     }\n",
    "#     if valid_dataloader is not None:\n",
    "#         net.eval()  # put network in train mode for Dropout and Batch Normalization\n",
    "#         valid_loss = torch.tensor(0., device=device)\n",
    "#         valid_accuracy = torch.tensor(0., device=device)\n",
    "#         with torch.no_grad():\n",
    "#             for X, y in valid_dataloader:\n",
    "#                 X = X.to(torch.float32).to(device)\n",
    "#                 y = y.to(torch.float32).to(device)\n",
    "#                 target = y.argmax(dim=1).to(torch.int64).to(device)\n",
    "#                 preds = net(X)\n",
    "#                 # print(preds.shape, y.shape)\n",
    "#                 # loss = criterion(preds, y)\n",
    "#                 _, pred = torch.max(preds, 1)\n",
    "#                 # print(pred)\n",
    "#                 # print(target)\n",
    "#                 # print(y)\n",
    "#                 correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "#                 correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "#                 # print(len(target))\n",
    "#                 # print(len(y))\n",
    "#                 for i in range(len(y)):\n",
    "#                     label = target.data[i]\n",
    "#                     class_correct[label] += correct[i].item()\n",
    "#                     class_total[label] += 1\n",
    "\n",
    "            \n",
    "                \n",
    "                \n",
    "#                 # valid_loss += loss * valid_dataloader.batch_size\n",
    "#                 valid_accuracy += (torch.argmax(preds, dim=1) == torch.argmax(y, dim=1)).sum()\n",
    "                \n",
    "#                 all_predictions = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "#                 all_targets = torch.argmax(y, dim=1).cpu().numpy()\n",
    "#                 precision = precision_score(all_targets, all_predictions, average=None)\n",
    "#                 recall = recall_score(all_targets, all_predictions, average=None)\n",
    "#                 f1 = f1_score(all_targets, all_predictions, average=None)\n",
    "                \n",
    "#                 results['precision'].append(precision)\n",
    "#                 results['recall'].append(recall)\n",
    "#                 results['f1'].append(f1)\n",
    "#                 results['predictions'].append(all_predictions)\n",
    "#                 results['targets'].append(all_targets)\n",
    "    \n",
    "#     if valid_dataloader is not None:\n",
    "#         # print(f'Valid loss: {valid_loss/len(valid_dataloader.dataset):.2f}')\n",
    "#         print(f'Valid accuracy: {100*valid_accuracy/len(valid_dataloader.dataset):.2f}')\n",
    "#     for i in range(len(classes)):\n",
    "#         if class_total[i] > 0:\n",
    "#             print(\"Test Accuracy of %5s: %2d%% (%2d/%2d)\" % (\n",
    "#                 classes[i], 100*class_correct[i]/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n",
    "#             ))\n",
    "#         else:\n",
    "#             print(\"Test accuracy of %5s: NA\" % (classes[i]))\n",
    "#     print(\"Test Accuracy of %2d%% (%2d/%2d)\" % (\n",
    "#                 100*np.sum(class_correct)/np.sum(class_total), np.sum(class_correct), np.sum(class_total)\n",
    "#             ))\n",
    "    \n",
    "#     end = time.time()\n",
    "#     print(f'Total training time: {end-start:.1f} seconds')\n",
    "#     return net, results\n",
    "\n",
    "# net, results = test(net, train_dataloader, valid_dataloader, criterion, optimizer, None, 1, device,chk=\"/Users/asmitganguly/Developer/internships/UWaterloo-24/code/model/kan/weights/oct/checkpoint.pth.tar\")\n",
    "\n",
    "# # print(results)\n",
    "\n",
    "\n",
    "# result_final = {\n",
    "#     'precision': [],\n",
    "#     'recall': [],\n",
    "#     'f1': [],\n",
    "#     'predictions': [],\n",
    "#     'targets': []\n",
    "# }\n",
    "\n",
    "# for k in results.keys():\n",
    "#     for ll in results[k]:\n",
    "#         for ele in ll:\n",
    "#             result_final[k].append(ele)\n",
    "            \n",
    "# # print(result_final)\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# cm = confusion_matrix(result_final[\"targets\"], result_final[\"predictions\"])\n",
    "\n",
    "# # display the confusion matrix as a heatmap\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "# plt.xlabel(\"Predicted labels\")\n",
    "# plt.ylabel(\"True labels\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115/115 [00:17<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0000\n",
      "Test Accuracy of Age_related_Macular_Degeneration: 100% (44/44)\n",
      "Test Accuracy of Central_serous: 98% (80/81)\n",
      "Test Accuracy of Diabetic: 100% (85/85)\n",
      "Test Accuracy of Macular_Hole: 100% (81/81)\n",
      "Test Accuracy of Normal: 100% (165/165)\n",
      "Test Accuracy of 99% (455/456)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "classes = list(pd.read_csv(\"/Users/asmitganguly/Developer/internships/UWaterloo-24/code/data/train_data.csv\")[\"class\"].unique())\n",
    "class_correct = list(0 for i in range(len(classes)))\n",
    "class_total = list(0 for i in range(len(classes)))\n",
    "net.eval()\n",
    "\n",
    "for data, target in tqdm(train_dataloader):\n",
    "    data, target = data.to(torch.float32).mean(dim=1, keepdim=True).to(device), target.argmax(dim=1).to(torch.int64).to(device)\n",
    "    with torch.no_grad(): # turn off autograd for faster testing\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "    test_loss = loss.item() * data.size(0)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # print(len(target))\n",
    "    if len(target) == 4:\n",
    "        for i in range(4):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "test_loss = test_loss / (len(train_dataloader)*4)\n",
    "print('Test Loss: {:.4f}'.format(test_loss))\n",
    "for i in range(len(classes)):\n",
    "    if class_total[i] > 0:\n",
    "        print(\"Test Accuracy of %5s: %2d%% (%2d/%2d)\" % (\n",
    "            classes[i], 100*class_correct[i]/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n",
    "        ))\n",
    "    else:\n",
    "        print(\"Test accuracy of %5s: NA\" % (classes[i]))\n",
    "print(\"Test Accuracy of %2d%% (%2d/%2d)\" % (\n",
    "            100*np.sum(class_correct)/np.sum(class_total), np.sum(class_correct), np.sum(class_total)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_projection(activation_batch):\n",
    "    # TBD: use pytorch batch svd implementation\n",
    "    activation_batch[np.isnan(activation_batch)] = 0\n",
    "    projections = []\n",
    "    for activations in activation_batch:\n",
    "        reshaped_activations = (activations).reshape(\n",
    "            activations.shape[0], -1).transpose()\n",
    "        # Centering before the SVD seems to be important here,\n",
    "        # Otherwise the image returned is negative\n",
    "        reshaped_activations = reshaped_activations - \\\n",
    "            reshaped_activations.mean(axis=0)\n",
    "        U, S, VT = np.linalg.svd(reshaped_activations, full_matrices=True)\n",
    "        projection = reshaped_activations @ VT[0, :]\n",
    "        projection = projection.reshape(activations.shape[1:])\n",
    "        projections.append(projection)\n",
    "    return np.float32(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor, label = TrainDataset.__getitem__(0)\n",
    "# input_tensor =  torch.tensor(input_tensor).to(torch.float32)\n",
    "# label =  torch.tensor(label).to(torch.float32)\n",
    "# input_tensor = input_tensor.unsqueeze(dim=0).mean(dim=1, keepdim=True).to(device)\n",
    "# act = model_feature.get_act(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # act = act.unsqueeze(dim=0)\n",
    "# for i in range(5):\n",
    "#     aa=act[:,i,:,:].cpu().detach().numpy()\n",
    "#     print(aa.shape)\n",
    "#     proj = get_2d_projection(aa)\n",
    "#     plt.imshow((aa*proj[:,np.newaxis,:])[0])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "# from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "# from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "# from torchvision.models import resnet50\n",
    "\n",
    "# # model = resnet50(pretrained=True)\n",
    "# target_layers = [net.model_feature.conv1]\n",
    "# input_tensor, label = TrainDataset.__getitem__(0)\n",
    "# input_tensor =  torch.tensor(input_tensor).to(torch.float32)\n",
    "# label =  torch.tensor(label).to(torch.float32)\n",
    "# # Note: input_tensor can be a batch tensor with several images!\n",
    "\n",
    "# # Construct the CAM object once, and then re-use it on many images:\n",
    "# # cam = GradCAM(model=net, target_layers=target_layers)\n",
    "# visuals = []\n",
    "\n",
    "# for cam in [GradCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad]:\n",
    "#     cam = cam(model=net, target_layers=target_layers)\n",
    "#     print(input_tensor.mean(dim=0).unsqueeze(dim=0).shape)\n",
    "#     grayscale_cam = cam(input_tensor=input_tensor.mean(dim=0).unsqueeze(dim=0), targets=None)\n",
    "#     grayscale_cam = grayscale_cam[0, :]\n",
    "#     array = input_tensor.numpy()\n",
    "#     array = np.transpose(array, (1, 2, 0))\n",
    "#     min_val, max_val = np.min(array), np.max(array)\n",
    "#     visualization = show_cam_on_image((array - min_val) / (max_val - min_val), grayscale_cam, use_rgb=False)\n",
    "#     visuals.append(visualization)\n",
    "    \n",
    "    \n",
    "# # cam = HiResCAM(model=net, target_layers=target_layers)\n",
    "\n",
    "# # You can also use it within a with statement, to make sure it is freed,\n",
    "# # In case you need to re-create it inside an outer loop:\n",
    "# # with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "# #   ...\n",
    "\n",
    "# # We have to specify the target we want to generate\n",
    "# # the Class Activation Maps for.\n",
    "# # If targets is None, the highest scoring category\n",
    "# # will be used for every image in the batch.\n",
    "# # Here we use ClassifierOutputTarget, but you can define your own custom targets\n",
    "# # That are, for example, combinations of categories, or specific outputs in a non standard model.\n",
    "\n",
    "# # targets = None\n",
    "# # # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "# # grayscale_cam = cam(input_tensor=input_tensor.unsqueeze(dim=0), targets=targets)\n",
    "\n",
    "# # # In this example grayscale_cam has only one image in the batch:\n",
    "# # grayscale_cam = grayscale_cam[0, :]\n",
    "# # array = input_tensor.numpy()\n",
    "# # array = np.transpose(array, (1, 2, 0))\n",
    "# # min_val, max_val = np.min(array), np.max(array)\n",
    "# # visualization = show_cam_on_image((array - min_val) / (max_val - min_val), grayscale_cam, use_rgb=False)\n",
    "\n",
    "# # # You can also get the model outputs without having to re-inference\n",
    "# # model_outputs = cam.outputs\n",
    "\n",
    "# # from matplotlib import pyplot as plt\n",
    "# # plt.imshow(visualization)\n",
    "# # plt.show()\n",
    "\n",
    "# for vis in visuals:\n",
    "    \n",
    "#     plt.imshow(vis)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from matplotlib import pyplot as plt\n",
    "# # plt.imshow(visualization)\n",
    "# # plt.show()\n",
    "\n",
    "# for vis in visuals:\n",
    "    \n",
    "#     plt.imshow(vis)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326570"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
